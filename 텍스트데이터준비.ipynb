{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement blueprints.exploration (from versions: none)\n",
      "ERROR: No matching distribution found for blueprints.exploration\n"
     ]
    }
   ],
   "source": [
    "pip install blueprints.exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textacy\n",
      "  Using cached textacy-0.13.0-py3-none-any.whl (210 kB)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (1.9.1)\n",
      "Collecting spacy~=3.0\n",
      "  Downloading spacy-3.6.0-cp39-cp39-win_amd64.whl (12.3 MB)\n",
      "     ---------------------------------------- 12.3/12.3 MB 6.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx>=2.7 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (2.8.4)\n",
      "Requirement already satisfied: tqdm>=4.19.6 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (4.64.1)\n",
      "Collecting catalogue~=2.0\n",
      "  Downloading catalogue-2.0.9-py3-none-any.whl (17 kB)\n",
      "Collecting floret~=0.10.0\n",
      "  Using cached floret-0.10.3-cp39-cp39-win_amd64.whl (233 kB)\n",
      "Collecting pyphen>=0.10.0\n",
      "  Using cached pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "Requirement already satisfied: jellyfish>=0.8.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (0.9.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (1.0.2)\n",
      "Requirement already satisfied: cachetools>=4.0.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (5.3.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (1.23.5)\n",
      "Requirement already satisfied: cytoolz>=0.10.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (0.11.0)\n",
      "Requirement already satisfied: joblib>=0.13.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (1.1.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from cytoolz>=0.10.1->textacy) (0.11.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from requests>=2.10.0->textacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from requests>=2.10.0->textacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from requests>=2.10.0->textacy) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from requests>=2.10.0->textacy) (1.26.11)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0->textacy) (2.2.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Using cached thinc-8.1.10-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.9-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.7-cp39-cp39-win_amd64.whl (483 kB)\n",
      "     ------------------------------------- 483.6/483.6 kB 10.1 MB/s eta 0:00:00\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.7-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from spacy~=3.0->textacy) (2.11.3)\n",
      "Collecting pathy>=0.10.0\n",
      "  Using cached pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.8-cp39-cp39-win_amd64.whl (96 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from spacy~=3.0->textacy) (5.2.1)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lg\\anaconda3\\lib\\site-packages (from spacy~=3.0->textacy) (63.4.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lg\\appdata\\roaming\\python\\python39\\site-packages (from spacy~=3.0->textacy) (23.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from spacy~=3.0->textacy) (1.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\lg\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.19.6->textacy) (0.4.6)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy~=3.0->textacy) (4.3.0)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.0-py3-none-any.whl (34 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.10-cp39-cp39-win_amd64.whl (7.4 MB)\n",
      "     ---------------------------------------- 7.4/7.4 MB 8.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy~=3.0->textacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from jinja2->spacy~=3.0->textacy) (2.0.1)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, pyphen, murmurhash, langcodes, floret, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy, textacy\n",
      "Successfully installed blis-0.7.10 catalogue-2.0.9 confection-0.1.0 cymem-2.0.7 floret-0.10.3 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.2 preshed-3.0.8 pyphen-0.14.0 spacy-3.6.0 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.7 textacy-0.13.0 thinc-8.1.10 typer-0.9.0 wasabi-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import html\n",
    "import textacy\n",
    "import textacy.preprocessing as tprep\n",
    "from textacy.preprocessing.resources import RE_URL\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0' : \n",
    "    raise SystemError('GPU device not found')\n",
    "print(f'Found GPU at: {device_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7027677138084711477\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2258003559\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7412294439032967442\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터 로드하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = pd.read_csv('./data/rspct_autos.tsv.gz', sep='\\t')\n",
    "subred_file = pd.read_csv('./data/subreddit_info.csv.gz').set_index(['subreddit'])\n",
    "\n",
    "df = post_df.join(subred_file, on='subreddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "      <td>autos</td>\n",
       "      <td>harley davidson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5s0q8r</td>\n",
       "      <td>Mustang</td>\n",
       "      <td>Roush vs Shleby GT500</td>\n",
       "      <td>I am trying to determine which is faster, and ...</td>\n",
       "      <td>autos</td>\n",
       "      <td>ford</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5z3405</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>2001 Golf Wagon looking for some insight</td>\n",
       "      <td>Hello! &lt;lb&gt;&lt;lb&gt;Trying to find some information...</td>\n",
       "      <td>autos</td>\n",
       "      <td>VW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7df18v</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>IS 250 Coolant Flush/Change</td>\n",
       "      <td>https://www.cars.com/articles/how-often-should...</td>\n",
       "      <td>autos</td>\n",
       "      <td>lexus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5tpve8</td>\n",
       "      <td>volt</td>\n",
       "      <td>Gen1 mpg w/ dead battery?</td>\n",
       "      <td>Hi, new to this subreddit.  I'm considering bu...</td>\n",
       "      <td>autos</td>\n",
       "      <td>chevrolet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   subreddit                                     title  \\\n",
       "0  8f73s7      Harley                            No Club Colors   \n",
       "1  5s0q8r     Mustang                     Roush vs Shleby GT500   \n",
       "2  5z3405  Volkswagen  2001 Golf Wagon looking for some insight   \n",
       "3  7df18v       Lexus               IS 250 Coolant Flush/Change   \n",
       "4  5tpve8        volt                 Gen1 mpg w/ dead battery?   \n",
       "\n",
       "                                            selftext category_1  \\\n",
       "0  Funny story. I went to college in Las Vegas. T...      autos   \n",
       "1  I am trying to determine which is faster, and ...      autos   \n",
       "2  Hello! <lb><lb>Trying to find some information...      autos   \n",
       "3  https://www.cars.com/articles/how-often-should...      autos   \n",
       "4  Hi, new to this subreddit.  I'm considering bu...      autos   \n",
       "\n",
       "        category_2 category_3  in_data reason_for_exclusion  \n",
       "0  harley davidson        NaN     True                  NaN  \n",
       "1             ford        NaN     True                  NaN  \n",
       "2               VW        NaN     True                  NaN  \n",
       "3            lexus        NaN     True                  NaN  \n",
       "4        chevrolet        NaN     True                  NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   id                    20000 non-null  object\n",
      " 1   subreddit             20000 non-null  object\n",
      " 2   title                 20000 non-null  object\n",
      " 3   selftext              20000 non-null  object\n",
      " 4   category_1            20000 non-null  object\n",
      " 5   category_2            20000 non-null  object\n",
      " 6   category_3            0 non-null      object\n",
      " 7   in_data               20000 non-null  bool  \n",
      " 8   reason_for_exclusion  0 non-null      object\n",
      "dtypes: bool(1), object(8)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    20000\n",
       "Name: in_data, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['in_data'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 속성 이름 표준화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'subreddit', 'title', 'selftext', 'category_1', 'category_2',\n",
      "       'category_3', 'in_data', 'reason_for_exclusion'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 열 목록 확인하기\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 현재 열 이름 새 이름으로 매핑하기\n",
    "# None으로 매핑된 열과 언급되지 않은 열은 삭제됨.\n",
    "column_mapping = {\n",
    "    'id': 'id',\n",
    "    'subreddit': 'subreddit',\n",
    "    'title': 'title',\n",
    "    'selftext': 'text',\n",
    "    'category_1': 'category',\n",
    "    'category_2': 'subcategory',\n",
    "    'category_3': None,           # 데이터가 없다.\n",
    "    'in_data': None,              # 필요없다.\n",
    "    'reason_for_exclusion': None  # 필요없다.\n",
    "}\n",
    "\n",
    "# 나머지 열들을 정의하기\n",
    "columns = [c for c in column_mapping.keys() if column_mapping[c] != None]\n",
    "\n",
    "# 열들을 선택하고 이름 바꾸기\n",
    "df = df[columns].rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "      <td>autos</td>\n",
       "      <td>harley davidson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5s0q8r</td>\n",
       "      <td>Mustang</td>\n",
       "      <td>Roush vs Shleby GT500</td>\n",
       "      <td>I am trying to determine which is faster, and ...</td>\n",
       "      <td>autos</td>\n",
       "      <td>ford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5z3405</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>2001 Golf Wagon looking for some insight</td>\n",
       "      <td>Hello! &lt;lb&gt;&lt;lb&gt;Trying to find some information...</td>\n",
       "      <td>autos</td>\n",
       "      <td>VW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7df18v</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>IS 250 Coolant Flush/Change</td>\n",
       "      <td>https://www.cars.com/articles/how-often-should...</td>\n",
       "      <td>autos</td>\n",
       "      <td>lexus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5tpve8</td>\n",
       "      <td>volt</td>\n",
       "      <td>Gen1 mpg w/ dead battery?</td>\n",
       "      <td>Hi, new to this subreddit.  I'm considering bu...</td>\n",
       "      <td>autos</td>\n",
       "      <td>chevrolet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   subreddit                                     title  \\\n",
       "0  8f73s7      Harley                            No Club Colors   \n",
       "1  5s0q8r     Mustang                     Roush vs Shleby GT500   \n",
       "2  5z3405  Volkswagen  2001 Golf Wagon looking for some insight   \n",
       "3  7df18v       Lexus               IS 250 Coolant Flush/Change   \n",
       "4  5tpve8        volt                 Gen1 mpg w/ dead battery?   \n",
       "\n",
       "                                                text category      subcategory  \n",
       "0  Funny story. I went to college in Las Vegas. T...    autos  harley davidson  \n",
       "1  I am trying to determine which is faster, and ...    autos             ford  \n",
       "2  Hello! <lb><lb>Trying to find some information...    autos               VW  \n",
       "3  https://www.cars.com/articles/how-often-should...    autos            lexus  \n",
       "4  Hi, new to this subreddit.  I'm considering bu...    autos        chevrolet  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터를 자동차 범주로 제한하기\n",
    "df = df[df['category'] == 'autos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>14356</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>7jc2k4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <td>volt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>Dashcam for 2017 volt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>Hello.&lt;lb&gt;I'm looking into getting a dashcam. &lt;lb&gt;Does anyone have any recommendations? &lt;lb&gt;&lt;lb&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subcategory</th>\n",
       "      <td>chevrolet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                           14356\n",
       "id                                                                                                        7jc2k4\n",
       "subreddit                                                                                                   volt\n",
       "title                                                                                      Dashcam for 2017 volt\n",
       "text         Hello.<lb>I'm looking into getting a dashcam. <lb>Does anyone have any recommendations? <lb><lb>...\n",
       "category                                                                                                   autos\n",
       "subcategory                                                                                            chevrolet"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.options.display.max_colwidth = None ###\n",
    "pd.options.display.max_colwidth = 100 ###\n",
    "df.sample(1, random_state=7).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 데이터프레임 저장 및 로드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('reddit_dataframe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'reddit-selfposts.db'\n",
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql('post', con, index=False, if_exists='replace')\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 프레임 복원하기\n",
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql('select * from post', con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 텍스트 데이터 정리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규 표현식으로 노이즈 식별"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)\n",
    "it got me thinking about the best match ups.\n",
    "<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)\n",
    "Captain America<lb>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)\n",
      "it got me thinking about the best match ups.\n",
      "<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)\n",
      "Captain America<lb>\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09009009009009009\n"
     ]
    }
   ],
   "source": [
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "\n",
    "def impurity(text, min_len = 10) : \n",
    "    '''텍스트에서 의심스러운 문자의 비율을 반환'''\n",
    "    if text == None or len(text) < min_len : \n",
    "        return 0 \n",
    "    else : \n",
    "        return len(RE_SUSPICIOUS.findall(text)) / len(text)\n",
    "    \n",
    "print(impurity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19682</th>\n",
       "      <td>Looking at buying a 335i with 39k miles and 11 months left on the CPO warranty. I asked the deal...</td>\n",
       "      <td>0.214716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12357</th>\n",
       "      <td>I'm looking to lease an a4 premium plus automatic with the nav package.&lt;lb&gt;&lt;lb&gt;Vehicle Price:&lt;ta...</td>\n",
       "      <td>0.165099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>Breakdown below:&lt;lb&gt;&lt;lb&gt;Elantra GT&lt;lb&gt;&lt;lb&gt;2.0L 4-cylinder&lt;lb&gt;&lt;lb&gt;6-speed Manual Transmission&lt;lb&gt;...</td>\n",
       "      <td>0.139130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      text  \\\n",
       "19682  Looking at buying a 335i with 39k miles and 11 months left on the CPO warranty. I asked the deal...   \n",
       "12357  I'm looking to lease an a4 premium plus automatic with the nav package.<lb><lb>Vehicle Price:<ta...   \n",
       "2730   Breakdown below:<lb><lb>Elantra GT<lb><lb>2.0L 4-cylinder<lb><lb>6-speed Manual Transmission<lb>...   \n",
       "\n",
       "       impurity  \n",
       "19682  0.214716  \n",
       "12357  0.165099  \n",
       "2730   0.139130  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 데이터프레임에 새 열 추가하기\n",
    "df['impurity'] = df['text'].apply(impurity, min_len=10)\n",
    "\n",
    "# 상위 3개 레코드 가져오기\n",
    "df[['text', 'impurity']].sort_values(by='impurity', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### count_words 구현"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(df, column='tokens', preprocess=None, min_freq=2) : \n",
    "    # 토큰 처리 및 counter 업데이트\n",
    "    def update(doc) : \n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # counter생성 및 모든 데이터에서 적용\n",
    "    counter = Counter()\n",
    "    df[column].progress_map(update)\n",
    "\n",
    "    # counter를 데이터프레임으로 전달(transform)\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:00<00:00, 145856.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;lb&gt;</th>\n",
       "      <td>100729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;tab&gt;</th>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         freq\n",
       "token        \n",
       "<lb>   100729\n",
       "<tab>     642"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정규 표현식으로 다른 태그가 있는지 확인하기\n",
    "# <[\\w/]*> : HTML과 유사한 구문 찾기(알파벳, 숫자, 밑줄 또는 /(슬래시)중 하나 찾기)\n",
    "count_words(df, column='text', preprocess=lambda t: re.findall(r'<[\\w/]*>', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규 표현식으로 노이즈 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text) : \n",
    "    # &amp와 같은 html 이스케이프를 문자로 변환한다.\n",
    "    text = html.unescape(text)\n",
    "    # <tap>과 같은 태그를 공백으로 변환하기\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # [Some text](https://...)와 같은 마크다운 URL을 공백으로 변환한다.\n",
    "    # r'\\1 : 첫 번째 그룹에 일치하는 내용 반환\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # [0]과 같은 괄호 안의 텍스트 또는 코드를 공백으로 변환한다.\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # 특수 문자로만 구성된 문자열을 공백으로 변환한다. \n",
    "    # &#은 변환되지만 #cool은 변환되지 않음.\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # --- 또는 == 같은 하이픈으로 이뤄진 문자열을 공백으로 변환한다.\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:$|\\s)', ' ', text)\n",
    "    # 연속된 공백을 공백 하나로 변환한다.\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After viewing the PINKIEPOOL Trailer it got me thinking about the best match ups. Here's my take: Deadpool Captain America\n",
      "Impurity: 0.0\n"
     ]
    }
   ],
   "source": [
    "# 샘플 텍스트에 clean함수 적용하고 결과 확인해보기\n",
    "clean_text = clean(text)\n",
    "print(clean_text)\n",
    "print('Impurity:', impurity(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14058</th>\n",
       "      <td>Mustang 2018, 2019, or 2020? Must Haves!! 1. Have a Credit score of 780\\+ for the best low inter...</td>\n",
       "      <td>0.030864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18934</th>\n",
       "      <td>At the dealership, they offered an option for foot-well illumination, but I cannot find any refe...</td>\n",
       "      <td>0.026455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16505</th>\n",
       "      <td>I am looking at four Caymans, all are in a similar price range. The major differences are the mi...</td>\n",
       "      <td>0.024631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                clean_text  \\\n",
       "14058  Mustang 2018, 2019, or 2020? Must Haves!! 1. Have a Credit score of 780\\+ for the best low inter...   \n",
       "18934  At the dealership, they offered an option for foot-well illumination, but I cannot find any refe...   \n",
       "16505  I am looking at four Caymans, all are in a similar price range. The major differences are the mi...   \n",
       "\n",
       "       impurity  \n",
       "14058  0.030864  \n",
       "18934  0.026455  \n",
       "16505  0.024631  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 정리된 텍스트 불순물을 다시 확인하고 \n",
    "# 필요하면 정리 단계 추가로 시행\n",
    "df['clean_text'] = df['text'].map(clean)\n",
    "df['impurity'] = df['clean_text'].apply(impurity, min_len=20)\n",
    "df[['clean_text','impurity']].sort_values(by='impurity', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### textacy를 사용한 문자 정규화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The café “Saint-Raphaël” is loca-\\nted on Côte dʼAzur.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다양한 하이픈과 따옴표를 표준화하고 악센트를 제거함\n",
    "def normalize(text) : \n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cafe \"Saint-Raphael\" is located on Cote d'Azur.\n"
     ]
    }
   ],
   "source": [
    "print(normalize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### textacy를 사용한 패턴 기반 데이터 마스킹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20000/20000 [00:01<00:00, 18584.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>www.getlowered.com</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://www.ecolamautomotive.com/#!2/kv7fq</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.reddit.com/r/Jeep/comments/4ux232/just_ordered_an_android_head_unit_joying_jeep/</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              freq\n",
       "token                                                                                             \n",
       "www.getlowered.com                                                                               3\n",
       "http://www.ecolamautomotive.com/#!2/kv7fq                                                        2\n",
       "https://www.reddit.com/r/Jeep/comments/4ux232/just_ordered_an_android_head_unit_joying_jeep/     2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 말뭉치에서 가장 자주 사용되는 url 찾기\n",
    "count_words(df, column='clean_text', preprocess=RE_URL.findall).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if textacy.__version__ < '0.11':\n",
    "    # as in book\n",
    "    replace_urls = textacy.preprocessing.replace_urls\n",
    "else:\n",
    "    replace_urls = textacy.preprocessing.replace.urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out _URL_\n"
     ]
    }
   ],
   "source": [
    "text = \"Check out https://spacy.io/usage/spacy-101\"\n",
    "\n",
    "# 대체할 때 대체값으로 _URL_을 사용한다.\n",
    "print(replace_urls(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 정규화 및 데이터 마스킹 기능을 데이터에 적용하기\n",
    "df['clean_text'] = df['clean_text'].map(replace_urls)\n",
    "df['clean_text'] = df['clean_text'].map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'text':'raw_text', 'clean_text':'text'}, inplace=True)\n",
    "df.drop(columns=['impurity'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터베이스에 저장\n",
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql('post_cleaned', con, index=False, if_exists='replace')\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling ...</td>\n",
       "      <td>autos</td>\n",
       "      <td>harley davidson</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5s0q8r</td>\n",
       "      <td>Mustang</td>\n",
       "      <td>Roush vs Shleby GT500</td>\n",
       "      <td>I am trying to determine which is faster, and I've seen the dealership video with the two racing...</td>\n",
       "      <td>autos</td>\n",
       "      <td>ford</td>\n",
       "      <td>I am trying to determine which is faster, and I've seen the dealership video with the two racing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5z3405</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>2001 Golf Wagon looking for some insight</td>\n",
       "      <td>Hello! &lt;lb&gt;&lt;lb&gt;Trying to find some information on replacing a 2001 Golf Wagon starter (gas).... ...</td>\n",
       "      <td>autos</td>\n",
       "      <td>VW</td>\n",
       "      <td>Hello! Trying to find some information on replacing a 2001 Golf Wagon starter (gas).... mine's g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7df18v</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>IS 250 Coolant Flush/Change</td>\n",
       "      <td>https://www.cars.com/articles/how-often-should-i-change-engine-coolant-1420680853669/&lt;lb&gt;&lt;lb&gt;I h...</td>\n",
       "      <td>autos</td>\n",
       "      <td>lexus</td>\n",
       "      <td>_URL_ I have a IS 250 AWD from 2006. About 73K miles on it. I've never touched the engine radiat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5tpve8</td>\n",
       "      <td>volt</td>\n",
       "      <td>Gen1 mpg w/ dead battery?</td>\n",
       "      <td>Hi, new to this subreddit.  I'm considering buying a Gen1 Volt, but I can't find any straight an...</td>\n",
       "      <td>autos</td>\n",
       "      <td>chevrolet</td>\n",
       "      <td>Hi, new to this subreddit. I'm considering buying a Gen1 Volt, but I can't find any straight ans...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   subreddit                                     title  \\\n",
       "0  8f73s7      Harley                            No Club Colors   \n",
       "1  5s0q8r     Mustang                     Roush vs Shleby GT500   \n",
       "2  5z3405  Volkswagen  2001 Golf Wagon looking for some insight   \n",
       "3  7df18v       Lexus               IS 250 Coolant Flush/Change   \n",
       "4  5tpve8        volt                 Gen1 mpg w/ dead battery?   \n",
       "\n",
       "                                                                                              raw_text  \\\n",
       "0  Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling ...   \n",
       "1  I am trying to determine which is faster, and I've seen the dealership video with the two racing...   \n",
       "2  Hello! <lb><lb>Trying to find some information on replacing a 2001 Golf Wagon starter (gas).... ...   \n",
       "3  https://www.cars.com/articles/how-often-should-i-change-engine-coolant-1420680853669/<lb><lb>I h...   \n",
       "4  Hi, new to this subreddit.  I'm considering buying a Gen1 Volt, but I can't find any straight an...   \n",
       "\n",
       "  category      subcategory  \\\n",
       "0    autos  harley davidson   \n",
       "1    autos             ford   \n",
       "2    autos               VW   \n",
       "3    autos            lexus   \n",
       "4    autos        chevrolet   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling ...  \n",
       "1  I am trying to determine which is faster, and I've seen the dealership video with the two racing...  \n",
       "2  Hello! Trying to find some information on replacing a 2001 Golf Wagon starter (gas).... mine's g...  \n",
       "3  _URL_ I have a IS 250 AWD from 2006. About 73K miles on it. I've never touched the engine radiat...  \n",
       "4  Hi, new to this subreddit. I'm considering buying a Gen1 Volt, but I can't find any straight ans...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "2019-08-10 23:32: @pete/@louis - I don't have a well-designed \n",
    "solution for today's problem. The code of module AC68 should be -1. \n",
    "Have to think a bit... #goodnight ;-) 😩😬\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 정규 표현식을 사용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019|08|10|23|32|pete|louis|don|have|well|designed|solution|for|today|problem|The|code|of|module|AC68|should|be|Have|to|think|bit|goodnight\n"
     ]
    }
   ],
   "source": [
    "tokens = re.findall(r'\\w\\w+', text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-10|23:32|:|@pete|@louis|I|don't|have|a|well-designed|solution|for|today's|problem|The|code|of|module|AC68|should|be|-1|Have|to|think|a|bit|#goodnight|;-)|😩|😬\n"
     ]
    }
   ],
   "source": [
    "# 이모티콘도 토큰화에 포함시키기\n",
    "RE_TOKEN = re.compile(r'''\n",
    "                      ([#]?[@\\w''\\.\\-\\:]*\\w       # 단어, 해시태크, 이메일 주소\n",
    "                      | [:;<]\\-[\\)\\(3]            # 폭넓게 설정한 기본 텍스트 이모지의 패턴\n",
    "                      | [\\U0001F1000-\\U0001FFFF]  # 폭넓게 설정한 이모지의 유니코드 범위\n",
    "                      )\n",
    "                      ''', re.VERBOSE)\n",
    "\n",
    "def tokenize(text) : \n",
    "    return RE_TOKEN.findall(text)\n",
    "\n",
    "tokens = tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTK를 사용한 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-10|23:32|:|@|pete/|@|louis|-|I|do|n't|have|a|well-designed|solution|for|today|'s|problem|.|The|code|of|module|AC68|should|be|-1|.|Have|to|think|a|bit|...|#|goodnight|;|-|)|😩😬\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-10|23:32|:|@pete|@louis|I|don't|have|a|well-designed|solution|for|today's|problem|The|code|of|module|AC68|should|be|-1|Have|to|think|a|bit|#goodnight|;-)|😩|😬\n"
     ]
    }
   ],
   "source": [
    "# Regex Tokenizer\n",
    "# RegexpTokenizer : re.findall()의 래퍼\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(RE_TOKEN.pattern, flags=re.VERBOSE)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(*tokens, sep='|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-10|23|:|32|:|@pete/@louis|-|I|don|'|t|have|a|well-designed|solution|for|today|'|s|problem.|The|code|of|module|AC68|should|be|-1.|Have|to|think|a|bit|...|#goodnight|;|-|)|😩😬\n"
     ]
    }
   ],
   "source": [
    "# Toktok Tokenizer : 다국어 관련 작업\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 스페이시를 활용한 언어 처리"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 파이프라인 인스턴스화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1c6ade830a0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1c6ade83520>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1c6b2e923c0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1c6b30e3c40>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1c6b30f3880>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1c6afbfed60>)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 파이프라인 구성요소 확인하기\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파서 및 개체명 인식 비활성화\n",
    "# 토큰화만 원하면 텍스트에서 nlp.make_doc 호출\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 텍스트 처리"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My|best|friend|Ryan|Peters|likes|fancy|adventure|games|.|"
     ]
    }
   ],
   "source": [
    "for token in doc : \n",
    "    print(token, end='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 토큰과 그 속성을 포함하는 테이블 생성하는 함수\n",
    "# lemma : 원형과 관련\n",
    "def display_nlp(doc, include_punct=False) : \n",
    "    '''스페이시 토큰들의 시각화를 위한 데이터프레임 생성하기'''\n",
    "    rows = []\n",
    "    for i, t in enumerate(doc) : \n",
    "        if not t.is_punct or include_punct : \n",
    "            row = {'token':i, 'text':t.text, 'lemma_':t.lemma_,\n",
    "                   'is_stop':t.is_stop, 'is_alpha':t.is_alpha,\n",
    "                   'pos_':t.pos_, 'dep_':t.dep_,\n",
    "                   'ent_type':t.ent_type_, 'ent_iob':t.ent_iob_}\n",
    "            rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index('token')\n",
    "    df.index.name = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>pos_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>ent_type</th>\n",
       "      <th>ent_iob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My</td>\n",
       "      <td>my</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>poss</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>friend</td>\n",
       "      <td>friend</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ryan</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peters</td>\n",
       "      <td>Peters</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>appos</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>likes</td>\n",
       "      <td>like</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fancy</td>\n",
       "      <td>fancy</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adventure</td>\n",
       "      <td>adventure</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>compound</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>games</td>\n",
       "      <td>game</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text     lemma_  is_stop  is_alpha   pos_      dep_ ent_type ent_iob\n",
       "0         My         my     True      True   PRON      poss                O\n",
       "1       best       good    False      True    ADJ      amod                O\n",
       "2     friend     friend    False      True   NOUN     nsubj                O\n",
       "3       Ryan       Ryan    False      True  PROPN  compound   PERSON       B\n",
       "4     Peters     Peters    False      True  PROPN     appos   PERSON       I\n",
       "5      likes       like    False      True   VERB      ROOT                O\n",
       "6      fancy      fancy    False      True    ADJ      amod                O\n",
       "7  adventure  adventure    False      True   NOUN  compound                O\n",
       "8      games       game    False      True   NOUN      dobj                O"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 사용자 전략 토큰화"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Pete|:|choose|low|-|carb|#|food|#|eat|-|smart|.|_|url|_|;-)|😋|👍|"
     ]
    }
   ],
   "source": [
    "# 예시\n",
    "text = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) 😋👍\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc : \n",
    "    print(token, end='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 중위, 접두사, 접미사 분할에 대한 개별 규칙 적용 함수\n",
    "def custom_tokenizer(nlp) : \n",
    "    # re.search와 일치하는 패턴을 제외하고 기본 패턴을 사용한다.\n",
    "    prefixes = [pattern for pattern in nlp.Defaults.prefixes \n",
    "                if pattern not in ['-', '_', '#']]\n",
    "    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
    "                if pattern not in ['_']]\n",
    "    infixes  = [pattern for pattern in nlp.Defaults.infixes\n",
    "                if not re.search(pattern, 'xx-xx')]\n",
    "\n",
    "    return Tokenizer(vocab          = nlp.vocab, \n",
    "                     rules          = nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search  = compile_prefix_regex(prefixes).search,\n",
    "                     suffix_search  = compile_suffix_regex(suffixes).search,\n",
    "                     infix_finditer = compile_infix_regex(infixes).finditer,\n",
    "                     token_match    = nlp.Defaults.token_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Pete|:|choose|low-carb|#food|#eat-smart|.|_url_|;-)|😋|👍|"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for token in doc : \n",
    "    print(token, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 불용어 제거"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dear, Ryan, need, sit, talk, Regards, Pete]\n"
     ]
    }
   ],
   "source": [
    "text = \"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
    "doc = nlp(text)\n",
    "\n",
    "non_stop = [t for t in doc if not t.is_stop and not t.is_punct]\n",
    "print(non_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.vocab['down'].is_stop = False\n",
    "nlp.vocab['Dear'].is_stop = True\n",
    "nlp.vocab['Regrads'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ryan, need, sit, down, talk, Regards, Pete]\n"
     ]
    }
   ],
   "source": [
    "text = \"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
    "doc = nlp(text)\n",
    "\n",
    "non_stop = [t for t in doc if not t.is_stop and not t.is_punct]\n",
    "print(non_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 해당 언어 클래스의 하위 클래스를 생성하고 거기에 자신의 불용어 목록 만들기(스페이시 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ryan|need|sit|down|talk|Pete|"
     ]
    }
   ],
   "source": [
    "excluded_stop_words = {'down'}\n",
    "included_stop_words = {'dear','regards'}\n",
    "\n",
    "class CustomEnglishDefaults(English.Defaults) : \n",
    "    stop_words = English.Defaults.stop_words.copy()\n",
    "    stop_words -= excluded_stop_words\n",
    "    stop_words |= included_stop_words # |= : in place set연산자(합집합을 구한 후 원래 집합에 결과 할당)\n",
    "\n",
    "class CustomEnglish(English) : \n",
    "    Defaults = CustomEnglishDefaults\n",
    "\n",
    "nlp = CustomEnglish()\n",
    "\n",
    "text = 'Dear Ryan, we need to sit down and talk. Regards, Pete'\n",
    "doc = nlp.make_doc(text)  # only tokenize\n",
    "\n",
    "tokens_wo_top = [token for token in doc]\n",
    "for token in doc : \n",
    "    if not token.is_stop and not token.is_punct : \n",
    "        print(token, end='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset nlp to original\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 품사 기반 원형 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my|good|friend|Ryan|Peters|like|fancy|adventure|game|.\n"
     ]
    }
   ],
   "source": [
    "# 원형 복원 : lemma_속성으로 접근 가능\n",
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(*[t.lemma_ for t in doc], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[friend, Ryan, Peters, adventure, games]\n"
     ]
    }
   ],
   "source": [
    "# 문서에 명사와 고유명사만 있는 목록 생성하기\n",
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "nouns = [t for t in doc if t.pos_ in ['NOUN','PROPN']]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best|friend|fancy|adventure|games\n"
     ]
    }
   ],
   "source": [
    "# 샘플 문장에서 형용사와 명사의 토큰 추출하기\n",
    "tokens = textacy.extract.words(doc,\n",
    "                               filter_stops = True,           # 기본은 True(불용어 허용 안함.)\n",
    "                               filter_punct = True,           # 기본은 True(구두점을 허용 안함.)\n",
    "                               filter_nums = True,            # 기본은 False\n",
    "                               include_pos = ['ADJ', 'NOUN'], # 기본은 None(모든 품사 허용)\n",
    "                               exclude_pos = None,            # 기본은 None(모든 품사를 배제하지 않음.)\n",
    "                               min_freq=1)\n",
    "\n",
    "print(*[t for t in tokens], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good|friend|fancy|adventure|game\n"
     ]
    }
   ],
   "source": [
    "# 단어의 원형 목록을 추출하는 함수\n",
    "def extract_lemmas(doc, **kwargs) : \n",
    "    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n",
    "\n",
    "lemmas = extract_lemmas(doc, include_pos=['ADJ','NOUN'])\n",
    "print(*lemmas, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 명사구 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good friend|fancy adventure|fancy adventure game\n"
     ]
    }
   ],
   "source": [
    "# 형용사가 앞에 있는 명사 시퀀스 추출하기\n",
    "text = 'My best friend Ryan Peters likes fancy adventure games.'\n",
    "doc = nlp(text)\n",
    "\n",
    "patterns = [\"POS:ADJ POS:NOUN:+\"]\n",
    "\n",
    "if textacy.__version__ < '0.11':\n",
    "    # as in book\n",
    "    spans = textacy.extract.matches(doc, patterns=patterns)\n",
    "else:\n",
    "    # new textacy version\n",
    "    spans = textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "    \n",
    "# spans = textacy.extract.matches(doc, patterns=patterns)\n",
    "print(*[s.lemma_ for s in spans], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My best friend|Ryan Peters|fancy adventure games\n"
     ]
    }
   ],
   "source": [
    "# 명사구 추출(대명사와 한정사도 포함될 수 있음)\n",
    "print(*doc.noun_chunks, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good_friend|fancy_adventure|fancy_adventure_game|adventure_game\n"
     ]
    }
   ],
   "source": [
    "# 품사 패턴을 기반으로 명사구 추출하기(구분 문자 사용)\n",
    "# 형용사나 명사 뒤에 명사가 오는 구절 추출하기\n",
    "def extract_noun_phrases(doc, preceding_pos=['NOUN'], sep='_'):\n",
    "    patterns = []\n",
    "    for pos in preceding_pos:\n",
    "        patterns.append(f\"POS:{pos} POS:NOUN:+\")\n",
    "\n",
    "    if textacy.__version__ < '0.11':\n",
    "        # as in book\n",
    "        spans = textacy.extract.matches(doc, patterns=patterns)\n",
    "    else:\n",
    "        # new textacy version\n",
    "        spans = textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "\n",
    "    return [sep.join([t.lemma_ for t in s]) for s in spans]\n",
    "\n",
    "print(*extract_noun_phrases(doc, ['ADJ', 'NOUN']), sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 개체명 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(James O'Neill, PERSON) (World Cargo Inc, ORG) (San Francisco, GPE) "
     ]
    }
   ],
   "source": [
    "# label_ : 개체 유형에 대한 속성\n",
    "text = \"James O'Neill, chairman of World Cargo Inc, lives in San Francisco.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents : \n",
    "    print(f'({ent.text}, {ent.label_})', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    James O'Neill\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", chairman of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    World Cargo Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", lives in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    San Francisco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displacy : 개체명 인식을 위한 시각화 제공\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특정 유형의 명명된 개체를 추출하기\n",
    "def extract_entities(doc, include_types=None, sep='_') : \n",
    "    ents = textacy.extract.entities(doc,\n",
    "                                    include_types=include_types,\n",
    "                                    exclude_types=None,\n",
    "                                    drop_determiners=True,\n",
    "                                    min_freq=1)\n",
    "    \n",
    "    return [sep.join([t.lemma_ for t in e])+'/'+e.label_ for e in ents]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
