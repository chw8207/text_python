{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement blueprints.exploration (from versions: none)\n",
      "ERROR: No matching distribution found for blueprints.exploration\n"
     ]
    }
   ],
   "source": [
    "pip install blueprints.exploration "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textacy\n",
      "  Using cached textacy-0.13.0-py3-none-any.whl (210 kB)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (1.9.1)\n",
      "Collecting spacy~=3.0\n",
      "  Downloading spacy-3.6.0-cp39-cp39-win_amd64.whl (12.3 MB)\n",
      "     ---------------------------------------- 12.3/12.3 MB 6.5 MB/s eta 0:00:00\n",
      "Requirement already satisfied: networkx>=2.7 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (2.8.4)\n",
      "Requirement already satisfied: tqdm>=4.19.6 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (4.64.1)\n",
      "Collecting catalogue~=2.0\n",
      "  Downloading catalogue-2.0.9-py3-none-any.whl (17 kB)\n",
      "Collecting floret~=0.10.0\n",
      "  Using cached floret-0.10.3-cp39-cp39-win_amd64.whl (233 kB)\n",
      "Collecting pyphen>=0.10.0\n",
      "  Using cached pyphen-0.14.0-py3-none-any.whl (2.0 MB)\n",
      "Requirement already satisfied: jellyfish>=0.8.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (0.9.0)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (1.0.2)\n",
      "Requirement already satisfied: cachetools>=4.0.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (5.3.1)\n",
      "Requirement already satisfied: requests>=2.10.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (2.28.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (1.23.5)\n",
      "Requirement already satisfied: cytoolz>=0.10.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (0.11.0)\n",
      "Requirement already satisfied: joblib>=0.13.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from textacy) (1.1.0)\n",
      "Requirement already satisfied: toolz>=0.8.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from cytoolz>=0.10.1->textacy) (0.11.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from requests>=2.10.0->textacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from requests>=2.10.0->textacy) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from requests>=2.10.0->textacy) (2022.9.14)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from requests>=2.10.0->textacy) (1.26.11)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from scikit-learn>=1.0->textacy) (2.2.0)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Using cached spacy_loggers-1.0.4-py3-none-any.whl (11 kB)\n",
      "Collecting thinc<8.2.0,>=8.1.8\n",
      "  Using cached thinc-8.1.10-cp39-cp39-win_amd64.whl (1.5 MB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0\n",
      "  Using cached murmurhash-1.0.9-cp39-cp39-win_amd64.whl (18 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3\n",
      "  Downloading srsly-2.4.7-cp39-cp39-win_amd64.whl (483 kB)\n",
      "     ------------------------------------- 483.6/483.6 kB 10.1 MB/s eta 0:00:00\n",
      "Collecting cymem<2.1.0,>=2.0.2\n",
      "  Using cached cymem-2.0.7-cp39-cp39-win_amd64.whl (30 kB)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from spacy~=3.0->textacy) (2.11.3)\n",
      "Collecting pathy>=0.10.0\n",
      "  Using cached pathy-0.10.2-py3-none-any.whl (48 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Using cached preshed-3.0.8-cp39-cp39-win_amd64.whl (96 kB)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from spacy~=3.0->textacy) (5.2.1)\n",
      "Collecting typer<0.10.0,>=0.3.0\n",
      "  Using cached typer-0.9.0-py3-none-any.whl (45 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\lg\\anaconda3\\lib\\site-packages (from spacy~=3.0->textacy) (63.4.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Using cached langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1\n",
      "  Using cached wasabi-1.1.2-py3-none-any.whl (27 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\lg\\appdata\\roaming\\python\\python39\\site-packages (from spacy~=3.0->textacy) (23.0)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from spacy~=3.0->textacy) (1.10.8)\n",
      "Requirement already satisfied: colorama in c:\\users\\lg\\appdata\\roaming\\python\\python39\\site-packages (from tqdm>=4.19.6->textacy) (0.4.6)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy~=3.0->textacy) (4.3.0)\n",
      "Collecting confection<1.0.0,>=0.0.1\n",
      "  Downloading confection-0.1.0-py3-none-any.whl (34 kB)\n",
      "Collecting blis<0.8.0,>=0.7.8\n",
      "  Downloading blis-0.7.10-cp39-cp39-win_amd64.whl (7.4 MB)\n",
      "     ---------------------------------------- 7.4/7.4 MB 8.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from typer<0.10.0,>=0.3.0->spacy~=3.0->textacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in c:\\users\\lg\\anaconda3\\lib\\site-packages (from jinja2->spacy~=3.0->textacy) (2.0.1)\n",
      "Installing collected packages: cymem, wasabi, spacy-loggers, spacy-legacy, pyphen, murmurhash, langcodes, floret, catalogue, blis, typer, srsly, preshed, pathy, confection, thinc, spacy, textacy\n",
      "Successfully installed blis-0.7.10 catalogue-2.0.9 confection-0.1.0 cymem-2.0.7 floret-0.10.3 langcodes-3.3.0 murmurhash-1.0.9 pathy-0.10.2 preshed-3.0.8 pyphen-0.14.0 spacy-3.6.0 spacy-legacy-3.0.12 spacy-loggers-1.0.4 srsly-2.4.7 textacy-0.13.0 thinc-8.1.10 typer-0.9.0 wasabi-1.1.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install textacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "import re\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "import html\n",
    "import textacy\n",
    "import textacy.preprocessing as tprep\n",
    "from textacy.preprocessing.resources import RE_URL\n",
    "import nltk\n",
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import compile_prefix_regex, compile_infix_regex, compile_suffix_regex\n",
    "from spacy.lang.en import English\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU ì„¤ì •"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found GPU at: /device:GPU:0\n"
     ]
    }
   ],
   "source": [
    "device_name = tf.test.gpu_device_name()\n",
    "if device_name != '/device:GPU:0' : \n",
    "    raise SystemError('GPU device not found')\n",
    "print(f'Found GPU at: {device_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 7027677138084711477\n",
      "xla_global_id: -1\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 2258003559\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7412294439032967442\n",
      "physical_device_desc: \"device: 0, name: NVIDIA GeForce GTX 1650 Ti, pci bus id: 0000:01:00.0, compute capability: 7.5\"\n",
      "xla_global_id: 416903419\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë°ì´í„° ë¡œë“œí•˜ê¸°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = pd.read_csv('./data/rspct_autos.tsv.gz', sep='\\t')\n",
    "subred_file = pd.read_csv('./data/subreddit_info.csv.gz').set_index(['subreddit'])\n",
    "\n",
    "df = post_df.join(subred_file, on='subreddit')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>selftext</th>\n",
       "      <th>category_1</th>\n",
       "      <th>category_2</th>\n",
       "      <th>category_3</th>\n",
       "      <th>in_data</th>\n",
       "      <th>reason_for_exclusion</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "      <td>autos</td>\n",
       "      <td>harley davidson</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5s0q8r</td>\n",
       "      <td>Mustang</td>\n",
       "      <td>Roush vs Shleby GT500</td>\n",
       "      <td>I am trying to determine which is faster, and ...</td>\n",
       "      <td>autos</td>\n",
       "      <td>ford</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5z3405</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>2001 Golf Wagon looking for some insight</td>\n",
       "      <td>Hello! &lt;lb&gt;&lt;lb&gt;Trying to find some information...</td>\n",
       "      <td>autos</td>\n",
       "      <td>VW</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7df18v</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>IS 250 Coolant Flush/Change</td>\n",
       "      <td>https://www.cars.com/articles/how-often-should...</td>\n",
       "      <td>autos</td>\n",
       "      <td>lexus</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5tpve8</td>\n",
       "      <td>volt</td>\n",
       "      <td>Gen1 mpg w/ dead battery?</td>\n",
       "      <td>Hi, new to this subreddit.  I'm considering bu...</td>\n",
       "      <td>autos</td>\n",
       "      <td>chevrolet</td>\n",
       "      <td>NaN</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   subreddit                                     title  \\\n",
       "0  8f73s7      Harley                            No Club Colors   \n",
       "1  5s0q8r     Mustang                     Roush vs Shleby GT500   \n",
       "2  5z3405  Volkswagen  2001 Golf Wagon looking for some insight   \n",
       "3  7df18v       Lexus               IS 250 Coolant Flush/Change   \n",
       "4  5tpve8        volt                 Gen1 mpg w/ dead battery?   \n",
       "\n",
       "                                            selftext category_1  \\\n",
       "0  Funny story. I went to college in Las Vegas. T...      autos   \n",
       "1  I am trying to determine which is faster, and ...      autos   \n",
       "2  Hello! <lb><lb>Trying to find some information...      autos   \n",
       "3  https://www.cars.com/articles/how-often-should...      autos   \n",
       "4  Hi, new to this subreddit.  I'm considering bu...      autos   \n",
       "\n",
       "        category_2 category_3  in_data reason_for_exclusion  \n",
       "0  harley davidson        NaN     True                  NaN  \n",
       "1             ford        NaN     True                  NaN  \n",
       "2               VW        NaN     True                  NaN  \n",
       "3            lexus        NaN     True                  NaN  \n",
       "4        chevrolet        NaN     True                  NaN  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 20000 entries, 0 to 19999\n",
      "Data columns (total 9 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   id                    20000 non-null  object\n",
      " 1   subreddit             20000 non-null  object\n",
      " 2   title                 20000 non-null  object\n",
      " 3   selftext              20000 non-null  object\n",
      " 4   category_1            20000 non-null  object\n",
      " 5   category_2            20000 non-null  object\n",
      " 6   category_3            0 non-null      object\n",
      " 7   in_data               20000 non-null  bool  \n",
      " 8   reason_for_exclusion  0 non-null      object\n",
      "dtypes: bool(1), object(8)\n",
      "memory usage: 1.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True    20000\n",
       "Name: in_data, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['in_data'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì†ì„± ì´ë¦„ í‘œì¤€í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'subreddit', 'title', 'selftext', 'category_1', 'category_2',\n",
      "       'category_3', 'in_data', 'reason_for_exclusion'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# ì—´ ëª©ë¡ í™•ì¸í•˜ê¸°\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í˜„ì¬ ì—´ ì´ë¦„ ìƒˆ ì´ë¦„ìœ¼ë¡œ ë§¤í•‘í•˜ê¸°\n",
    "# Noneìœ¼ë¡œ ë§¤í•‘ëœ ì—´ê³¼ ì–¸ê¸‰ë˜ì§€ ì•Šì€ ì—´ì€ ì‚­ì œë¨.\n",
    "column_mapping = {\n",
    "    'id': 'id',\n",
    "    'subreddit': 'subreddit',\n",
    "    'title': 'title',\n",
    "    'selftext': 'text',\n",
    "    'category_1': 'category',\n",
    "    'category_2': 'subcategory',\n",
    "    'category_3': None,           # ë°ì´í„°ê°€ ì—†ë‹¤.\n",
    "    'in_data': None,              # í•„ìš”ì—†ë‹¤.\n",
    "    'reason_for_exclusion': None  # í•„ìš”ì—†ë‹¤.\n",
    "}\n",
    "\n",
    "# ë‚˜ë¨¸ì§€ ì—´ë“¤ì„ ì •ì˜í•˜ê¸°\n",
    "columns = [c for c in column_mapping.keys() if column_mapping[c] != None]\n",
    "\n",
    "# ì—´ë“¤ì„ ì„ íƒí•˜ê³  ì´ë¦„ ë°”ê¾¸ê¸°\n",
    "df = df[columns].rename(columns=column_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. T...</td>\n",
       "      <td>autos</td>\n",
       "      <td>harley davidson</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5s0q8r</td>\n",
       "      <td>Mustang</td>\n",
       "      <td>Roush vs Shleby GT500</td>\n",
       "      <td>I am trying to determine which is faster, and ...</td>\n",
       "      <td>autos</td>\n",
       "      <td>ford</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5z3405</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>2001 Golf Wagon looking for some insight</td>\n",
       "      <td>Hello! &lt;lb&gt;&lt;lb&gt;Trying to find some information...</td>\n",
       "      <td>autos</td>\n",
       "      <td>VW</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7df18v</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>IS 250 Coolant Flush/Change</td>\n",
       "      <td>https://www.cars.com/articles/how-often-should...</td>\n",
       "      <td>autos</td>\n",
       "      <td>lexus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5tpve8</td>\n",
       "      <td>volt</td>\n",
       "      <td>Gen1 mpg w/ dead battery?</td>\n",
       "      <td>Hi, new to this subreddit.  I'm considering bu...</td>\n",
       "      <td>autos</td>\n",
       "      <td>chevrolet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   subreddit                                     title  \\\n",
       "0  8f73s7      Harley                            No Club Colors   \n",
       "1  5s0q8r     Mustang                     Roush vs Shleby GT500   \n",
       "2  5z3405  Volkswagen  2001 Golf Wagon looking for some insight   \n",
       "3  7df18v       Lexus               IS 250 Coolant Flush/Change   \n",
       "4  5tpve8        volt                 Gen1 mpg w/ dead battery?   \n",
       "\n",
       "                                                text category      subcategory  \n",
       "0  Funny story. I went to college in Las Vegas. T...    autos  harley davidson  \n",
       "1  I am trying to determine which is faster, and ...    autos             ford  \n",
       "2  Hello! <lb><lb>Trying to find some information...    autos               VW  \n",
       "3  https://www.cars.com/articles/how-often-should...    autos            lexus  \n",
       "4  Hi, new to this subreddit.  I'm considering bu...    autos        chevrolet  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ë¥¼ ìë™ì°¨ ë²”ì£¼ë¡œ ì œí•œí•˜ê¸°\n",
    "df = df[df['category'] == 'autos']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>14356</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>7jc2k4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subreddit</th>\n",
       "      <td>volt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>Dashcam for 2017 volt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>text</th>\n",
       "      <td>Hello.&lt;lb&gt;I'm looking into getting a dashcam. &lt;lb&gt;Does anyone have any recommendations? &lt;lb&gt;&lt;lb&gt;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>category</th>\n",
       "      <td>autos</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>subcategory</th>\n",
       "      <td>chevrolet</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                           14356\n",
       "id                                                                                                        7jc2k4\n",
       "subreddit                                                                                                   volt\n",
       "title                                                                                      Dashcam for 2017 volt\n",
       "text         Hello.<lb>I'm looking into getting a dashcam. <lb>Does anyone have any recommendations? <lb><lb>...\n",
       "category                                                                                                   autos\n",
       "subcategory                                                                                            chevrolet"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pd.options.display.max_colwidth = None ###\n",
    "pd.options.display.max_colwidth = 100 ###\n",
    "df.sample(1, random_state=7).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë°ì´í„°í”„ë ˆì„ ì €ì¥ ë° ë¡œë“œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_pickle('reddit_dataframe.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_name = 'reddit-selfposts.db'\n",
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql('post', con, index=False, if_exists='replace')\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„° í”„ë ˆì„ ë³µì›í•˜ê¸°\n",
    "con = sqlite3.connect(db_name)\n",
    "df = pd.read_sql('select * from post', con)\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í…ìŠ¤íŠ¸ ë°ì´í„° ì •ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì‹ë³„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)\n",
    "it got me thinking about the best match ups.\n",
    "<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)\n",
    "Captain America<lb>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After viewing the [PINKIEPOOL Trailer](https://www.youtu.be/watch?v=ieHRoHUg)\n",
      "it got me thinking about the best match ups.\n",
      "<lb>Here's my take:<lb><lb>[](/sp)[](/ppseesyou) Deadpool<lb>[](/sp)[](/ajsly)\n",
      "Captain America<lb>\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09009009009009009\n"
     ]
    }
   ],
   "source": [
    "RE_SUSPICIOUS = re.compile(r'[&#<>{}\\[\\]\\\\]')\n",
    "\n",
    "def impurity(text, min_len = 10) : \n",
    "    '''í…ìŠ¤íŠ¸ì—ì„œ ì˜ì‹¬ìŠ¤ëŸ¬ìš´ ë¬¸ìì˜ ë¹„ìœ¨ì„ ë°˜í™˜'''\n",
    "    if text == None or len(text) < min_len : \n",
    "        return 0 \n",
    "    else : \n",
    "        return len(RE_SUSPICIOUS.findall(text)) / len(text)\n",
    "    \n",
    "print(impurity(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19682</th>\n",
       "      <td>Looking at buying a 335i with 39k miles and 11 months left on the CPO warranty. I asked the deal...</td>\n",
       "      <td>0.214716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12357</th>\n",
       "      <td>I'm looking to lease an a4 premium plus automatic with the nav package.&lt;lb&gt;&lt;lb&gt;Vehicle Price:&lt;ta...</td>\n",
       "      <td>0.165099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2730</th>\n",
       "      <td>Breakdown below:&lt;lb&gt;&lt;lb&gt;Elantra GT&lt;lb&gt;&lt;lb&gt;2.0L 4-cylinder&lt;lb&gt;&lt;lb&gt;6-speed Manual Transmission&lt;lb&gt;...</td>\n",
       "      <td>0.139130</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                      text  \\\n",
       "19682  Looking at buying a 335i with 39k miles and 11 months left on the CPO warranty. I asked the deal...   \n",
       "12357  I'm looking to lease an a4 premium plus automatic with the nav package.<lb><lb>Vehicle Price:<ta...   \n",
       "2730   Breakdown below:<lb><lb>Elantra GT<lb><lb>2.0L 4-cylinder<lb><lb>6-speed Manual Transmission<lb>...   \n",
       "\n",
       "       impurity  \n",
       "19682  0.214716  \n",
       "12357  0.165099  \n",
       "2730   0.139130  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë°ì´í„°í”„ë ˆì„ì— ìƒˆ ì—´ ì¶”ê°€í•˜ê¸°\n",
    "df['impurity'] = df['text'].apply(impurity, min_len=10)\n",
    "\n",
    "# ìƒìœ„ 3ê°œ ë ˆì½”ë“œ ê°€ì ¸ì˜¤ê¸°\n",
    "df[['text', 'impurity']].sort_values(by='impurity', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### count_words êµ¬í˜„"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(df, column='tokens', preprocess=None, min_freq=2) : \n",
    "    # í† í° ì²˜ë¦¬ ë° counter ì—…ë°ì´íŠ¸\n",
    "    def update(doc) : \n",
    "        tokens = doc if preprocess is None else preprocess(doc)\n",
    "        counter.update(tokens)\n",
    "\n",
    "    # counterìƒì„± ë° ëª¨ë“  ë°ì´í„°ì—ì„œ ì ìš©\n",
    "    counter = Counter()\n",
    "    df[column].progress_map(update)\n",
    "\n",
    "    # counterë¥¼ ë°ì´í„°í”„ë ˆì„ìœ¼ë¡œ ì „ë‹¬(transform)\n",
    "    freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['freq'])\n",
    "    freq_df = freq_df.query('freq >= @min_freq')\n",
    "    freq_df.index.name = 'token'\n",
    "    return freq_df.sort_values('freq', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:00<00:00, 145856.87it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>&lt;lb&gt;</th>\n",
       "      <td>100729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>&lt;tab&gt;</th>\n",
       "      <td>642</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         freq\n",
       "token        \n",
       "<lb>   100729\n",
       "<tab>     642"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ë‹¤ë¥¸ íƒœê·¸ê°€ ìˆëŠ”ì§€ í™•ì¸í•˜ê¸°\n",
    "# <[\\w/]*> : HTMLê³¼ ìœ ì‚¬í•œ êµ¬ë¬¸ ì°¾ê¸°(ì•ŒíŒŒë²³, ìˆ«ì, ë°‘ì¤„ ë˜ëŠ” /(ìŠ¬ë˜ì‹œ)ì¤‘ í•˜ë‚˜ ì°¾ê¸°)\n",
    "count_words(df, column='text', preprocess=lambda t: re.findall(r'<[\\w/]*>', t))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì •ê·œ í‘œí˜„ì‹ìœ¼ë¡œ ë…¸ì´ì¦ˆ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(text) : \n",
    "    # &ampì™€ ê°™ì€ html ì´ìŠ¤ì¼€ì´í”„ë¥¼ ë¬¸ìë¡œ ë³€í™˜í•œë‹¤.\n",
    "    text = html.unescape(text)\n",
    "    # <tap>ê³¼ ê°™ì€ íƒœê·¸ë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•˜ê¸°\n",
    "    text = re.sub(r'<[^<>]*>', ' ', text)\n",
    "    # [Some text](https://...)ì™€ ê°™ì€ ë§ˆí¬ë‹¤ìš´ URLì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•œë‹¤.\n",
    "    # r'\\1 : ì²« ë²ˆì§¸ ê·¸ë£¹ì— ì¼ì¹˜í•˜ëŠ” ë‚´ìš© ë°˜í™˜\n",
    "    text = re.sub(r'\\[([^\\[\\]]*)\\]\\([^\\(\\)]*\\)', r'\\1', text)\n",
    "    # [0]ê³¼ ê°™ì€ ê´„í˜¸ ì•ˆì˜ í…ìŠ¤íŠ¸ ë˜ëŠ” ì½”ë“œë¥¼ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•œë‹¤.\n",
    "    text = re.sub(r'\\[[^\\[\\]]*\\]', ' ', text)\n",
    "    # íŠ¹ìˆ˜ ë¬¸ìë¡œë§Œ êµ¬ì„±ëœ ë¬¸ìì—´ì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•œë‹¤. \n",
    "    # &#ì€ ë³€í™˜ë˜ì§€ë§Œ #coolì€ ë³€í™˜ë˜ì§€ ì•ŠìŒ.\n",
    "    text = re.sub(r'(?:^|\\s)[&#<>{}\\[\\]+|\\\\:-]{1,}(?:\\s|$)', ' ', text)\n",
    "    # --- ë˜ëŠ” == ê°™ì€ í•˜ì´í”ˆìœ¼ë¡œ ì´ë¤„ì§„ ë¬¸ìì—´ì„ ê³µë°±ìœ¼ë¡œ ë³€í™˜í•œë‹¤.\n",
    "    text = re.sub(r'(?:^|\\s)[\\-=\\+]{2,}(?:$|\\s)', ' ', text)\n",
    "    # ì—°ì†ëœ ê³µë°±ì„ ê³µë°± í•˜ë‚˜ë¡œ ë³€í™˜í•œë‹¤.\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After viewing the PINKIEPOOL Trailer it got me thinking about the best match ups. Here's my take: Deadpool Captain America\n",
      "Impurity: 0.0\n"
     ]
    }
   ],
   "source": [
    "# ìƒ˜í”Œ í…ìŠ¤íŠ¸ì— cleaní•¨ìˆ˜ ì ìš©í•˜ê³  ê²°ê³¼ í™•ì¸í•´ë³´ê¸°\n",
    "clean_text = clean(text)\n",
    "print(clean_text)\n",
    "print('Impurity:', impurity(clean_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>impurity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14058</th>\n",
       "      <td>Mustang 2018, 2019, or 2020? Must Haves!! 1. Have a Credit score of 780\\+ for the best low inter...</td>\n",
       "      <td>0.030864</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18934</th>\n",
       "      <td>At the dealership, they offered an option for foot-well illumination, but I cannot find any refe...</td>\n",
       "      <td>0.026455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16505</th>\n",
       "      <td>I am looking at four Caymans, all are in a similar price range. The major differences are the mi...</td>\n",
       "      <td>0.024631</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                clean_text  \\\n",
       "14058  Mustang 2018, 2019, or 2020? Must Haves!! 1. Have a Credit score of 780\\+ for the best low inter...   \n",
       "18934  At the dealership, they offered an option for foot-well illumination, but I cannot find any refe...   \n",
       "16505  I am looking at four Caymans, all are in a similar price range. The major differences are the mi...   \n",
       "\n",
       "       impurity  \n",
       "14058  0.030864  \n",
       "18934  0.026455  \n",
       "16505  0.024631  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ì •ë¦¬ëœ í…ìŠ¤íŠ¸ ë¶ˆìˆœë¬¼ì„ ë‹¤ì‹œ í™•ì¸í•˜ê³  \n",
    "# í•„ìš”í•˜ë©´ ì •ë¦¬ ë‹¨ê³„ ì¶”ê°€ë¡œ ì‹œí–‰\n",
    "df['clean_text'] = df['text'].map(clean)\n",
    "df['impurity'] = df['clean_text'].apply(impurity, min_len=20)\n",
    "df[['clean_text','impurity']].sort_values(by='impurity', ascending=False).head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### textacyë¥¼ ì‚¬ìš©í•œ ë¬¸ì ì •ê·œí™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"The cafÃ© â€œSaint-RaphaÃ«lâ€ is loca-\\nted on CÃ´te dÊ¼Azur.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë‹¤ì–‘í•œ í•˜ì´í”ˆê³¼ ë”°ì˜´í‘œë¥¼ í‘œì¤€í™”í•˜ê³  ì•…ì„¼íŠ¸ë¥¼ ì œê±°í•¨\n",
    "def normalize(text) : \n",
    "    text = tprep.normalize.hyphenated_words(text)\n",
    "    text = tprep.normalize.quotation_marks(text)\n",
    "    text = tprep.normalize.unicode(text)\n",
    "    text = tprep.remove.accents(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The cafe \"Saint-Raphael\" is located on Cote d'Azur.\n"
     ]
    }
   ],
   "source": [
    "print(normalize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### textacyë¥¼ ì‚¬ìš©í•œ íŒ¨í„´ ê¸°ë°˜ ë°ì´í„° ë§ˆìŠ¤í‚¹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 20000/20000 [00:01<00:00, 18584.73it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>token</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>www.getlowered.com</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>http://www.ecolamautomotive.com/#!2/kv7fq</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>https://www.reddit.com/r/Jeep/comments/4ux232/just_ordered_an_android_head_unit_joying_jeep/</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                              freq\n",
       "token                                                                                             \n",
       "www.getlowered.com                                                                               3\n",
       "http://www.ecolamautomotive.com/#!2/kv7fq                                                        2\n",
       "https://www.reddit.com/r/Jeep/comments/4ux232/just_ordered_an_android_head_unit_joying_jeep/     2"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ë§ë­‰ì¹˜ì—ì„œ ê°€ì¥ ìì£¼ ì‚¬ìš©ë˜ëŠ” url ì°¾ê¸°\n",
    "count_words(df, column='clean_text', preprocess=RE_URL.findall).head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "if textacy.__version__ < '0.11':\n",
    "    # as in book\n",
    "    replace_urls = textacy.preprocessing.replace_urls\n",
    "else:\n",
    "    replace_urls = textacy.preprocessing.replace.urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check out _URL_\n"
     ]
    }
   ],
   "source": [
    "text = \"Check out https://spacy.io/usage/spacy-101\"\n",
    "\n",
    "# ëŒ€ì²´í•  ë•Œ ëŒ€ì²´ê°’ìœ¼ë¡œ _URL_ì„ ì‚¬ìš©í•œë‹¤.\n",
    "print(replace_urls(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì •ê·œí™” ë° ë°ì´í„° ë§ˆìŠ¤í‚¹ ê¸°ëŠ¥ì„ ë°ì´í„°ì— ì ìš©í•˜ê¸°\n",
    "df['clean_text'] = df['clean_text'].map(replace_urls)\n",
    "df['clean_text'] = df['clean_text'].map(normalize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.rename(columns={'text':'raw_text', 'clean_text':'text'}, inplace=True)\n",
    "df.drop(columns=['impurity'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ë°ì´í„°ë² ì´ìŠ¤ì— ì €ì¥\n",
    "con = sqlite3.connect(db_name)\n",
    "df.to_sql('post_cleaned', con, index=False, if_exists='replace')\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>subreddit</th>\n",
       "      <th>title</th>\n",
       "      <th>raw_text</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8f73s7</td>\n",
       "      <td>Harley</td>\n",
       "      <td>No Club Colors</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling ...</td>\n",
       "      <td>autos</td>\n",
       "      <td>harley davidson</td>\n",
       "      <td>Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5s0q8r</td>\n",
       "      <td>Mustang</td>\n",
       "      <td>Roush vs Shleby GT500</td>\n",
       "      <td>I am trying to determine which is faster, and I've seen the dealership video with the two racing...</td>\n",
       "      <td>autos</td>\n",
       "      <td>ford</td>\n",
       "      <td>I am trying to determine which is faster, and I've seen the dealership video with the two racing...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5z3405</td>\n",
       "      <td>Volkswagen</td>\n",
       "      <td>2001 Golf Wagon looking for some insight</td>\n",
       "      <td>Hello! &lt;lb&gt;&lt;lb&gt;Trying to find some information on replacing a 2001 Golf Wagon starter (gas).... ...</td>\n",
       "      <td>autos</td>\n",
       "      <td>VW</td>\n",
       "      <td>Hello! Trying to find some information on replacing a 2001 Golf Wagon starter (gas).... mine's g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7df18v</td>\n",
       "      <td>Lexus</td>\n",
       "      <td>IS 250 Coolant Flush/Change</td>\n",
       "      <td>https://www.cars.com/articles/how-often-should-i-change-engine-coolant-1420680853669/&lt;lb&gt;&lt;lb&gt;I h...</td>\n",
       "      <td>autos</td>\n",
       "      <td>lexus</td>\n",
       "      <td>_URL_ I have a IS 250 AWD from 2006. About 73K miles on it. I've never touched the engine radiat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5tpve8</td>\n",
       "      <td>volt</td>\n",
       "      <td>Gen1 mpg w/ dead battery?</td>\n",
       "      <td>Hi, new to this subreddit.  I'm considering buying a Gen1 Volt, but I can't find any straight an...</td>\n",
       "      <td>autos</td>\n",
       "      <td>chevrolet</td>\n",
       "      <td>Hi, new to this subreddit. I'm considering buying a Gen1 Volt, but I can't find any straight ans...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id   subreddit                                     title  \\\n",
       "0  8f73s7      Harley                            No Club Colors   \n",
       "1  5s0q8r     Mustang                     Roush vs Shleby GT500   \n",
       "2  5z3405  Volkswagen  2001 Golf Wagon looking for some insight   \n",
       "3  7df18v       Lexus               IS 250 Coolant Flush/Change   \n",
       "4  5tpve8        volt                 Gen1 mpg w/ dead battery?   \n",
       "\n",
       "                                                                                              raw_text  \\\n",
       "0  Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling ...   \n",
       "1  I am trying to determine which is faster, and I've seen the dealership video with the two racing...   \n",
       "2  Hello! <lb><lb>Trying to find some information on replacing a 2001 Golf Wagon starter (gas).... ...   \n",
       "3  https://www.cars.com/articles/how-often-should-i-change-engine-coolant-1420680853669/<lb><lb>I h...   \n",
       "4  Hi, new to this subreddit.  I'm considering buying a Gen1 Volt, but I can't find any straight an...   \n",
       "\n",
       "  category      subcategory  \\\n",
       "0    autos  harley davidson   \n",
       "1    autos             ford   \n",
       "2    autos               VW   \n",
       "3    autos            lexus   \n",
       "4    autos        chevrolet   \n",
       "\n",
       "                                                                                                  text  \n",
       "0  Funny story. I went to college in Las Vegas. This was before I knew anything about motorcycling ...  \n",
       "1  I am trying to determine which is faster, and I've seen the dealership video with the two racing...  \n",
       "2  Hello! Trying to find some information on replacing a 2001 Golf Wagon starter (gas).... mine's g...  \n",
       "3  _URL_ I have a IS 250 AWD from 2006. About 73K miles on it. I've never touched the engine radiat...  \n",
       "4  Hi, new to this subreddit. I'm considering buying a Gen1 Volt, but I can't find any straight ans...  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "2019-08-10 23:32: @pete/@louis - I don't have a well-designed \n",
    "solution for today's problem. The code of module AC68 should be -1. \n",
    "Have to think a bit... #goodnight ;-) ğŸ˜©ğŸ˜¬\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì •ê·œ í‘œí˜„ì‹ì„ ì‚¬ìš©í•œ í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019|08|10|23|32|pete|louis|don|have|well|designed|solution|for|today|problem|The|code|of|module|AC68|should|be|Have|to|think|bit|goodnight\n"
     ]
    }
   ],
   "source": [
    "tokens = re.findall(r'\\w\\w+', text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-10|23:32|:|@pete|@louis|I|don't|have|a|well-designed|solution|for|today's|problem|The|code|of|module|AC68|should|be|-1|Have|to|think|a|bit|#goodnight|;-)|ğŸ˜©|ğŸ˜¬\n"
     ]
    }
   ],
   "source": [
    "# ì´ëª¨í‹°ì½˜ë„ í† í°í™”ì— í¬í•¨ì‹œí‚¤ê¸°\n",
    "RE_TOKEN = re.compile(r'''\n",
    "                      ([#]?[@\\w''\\.\\-\\:]*\\w       # ë‹¨ì–´, í•´ì‹œíƒœí¬, ì´ë©”ì¼ ì£¼ì†Œ\n",
    "                      | [:;<]\\-[\\)\\(3]            # í­ë„“ê²Œ ì„¤ì •í•œ ê¸°ë³¸ í…ìŠ¤íŠ¸ ì´ëª¨ì§€ì˜ íŒ¨í„´\n",
    "                      | [\\U0001F1000-\\U0001FFFF]  # í­ë„“ê²Œ ì„¤ì •í•œ ì´ëª¨ì§€ì˜ ìœ ë‹ˆì½”ë“œ ë²”ìœ„\n",
    "                      )\n",
    "                      ''', re.VERBOSE)\n",
    "\n",
    "def tokenize(text) : \n",
    "    return RE_TOKEN.findall(text)\n",
    "\n",
    "tokens = tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NLTKë¥¼ ì‚¬ìš©í•œ í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-10|23:32|:|@|pete/|@|louis|-|I|do|n't|have|a|well-designed|solution|for|today|'s|problem|.|The|code|of|module|AC68|should|be|-1|.|Have|to|think|a|bit|...|#|goodnight|;|-|)|ğŸ˜©ğŸ˜¬\n"
     ]
    }
   ],
   "source": [
    "tokens = nltk.tokenize.word_tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-10|23:32|:|@pete|@louis|I|don't|have|a|well-designed|solution|for|today's|problem|The|code|of|module|AC68|should|be|-1|Have|to|think|a|bit|#goodnight|;-)|ğŸ˜©|ğŸ˜¬\n"
     ]
    }
   ],
   "source": [
    "# Regex Tokenizer\n",
    "# RegexpTokenizer : re.findall()ì˜ ë˜í¼\n",
    "tokenizer = nltk.tokenize.RegexpTokenizer(RE_TOKEN.pattern, flags=re.VERBOSE)\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(*tokens, sep='|')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-08-10|23|:|32|:|@pete/@louis|-|I|don|'|t|have|a|well-designed|solution|for|today|'|s|problem.|The|code|of|module|AC68|should|be|-1.|Have|to|think|a|bit|...|#goodnight|;|-|)|ğŸ˜©ğŸ˜¬\n"
     ]
    }
   ],
   "source": [
    "# Toktok Tokenizer : ë‹¤êµ­ì–´ ê´€ë ¨ ì‘ì—…\n",
    "tokenizer = nltk.tokenize.ToktokTokenizer()\n",
    "tokens = tokenizer.tokenize(text)\n",
    "print(*tokens, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ìŠ¤í˜ì´ì‹œë¥¼ í™œìš©í•œ ì–¸ì–´ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### íŒŒì´í”„ë¼ì¸ ì¸ìŠ¤í„´ìŠ¤í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('tok2vec', <spacy.pipeline.tok2vec.Tok2Vec at 0x1c6ade830a0>),\n",
       " ('tagger', <spacy.pipeline.tagger.Tagger at 0x1c6ade83520>),\n",
       " ('parser', <spacy.pipeline.dep_parser.DependencyParser at 0x1c6b2e923c0>),\n",
       " ('attribute_ruler',\n",
       "  <spacy.pipeline.attributeruler.AttributeRuler at 0x1c6b30e3c40>),\n",
       " ('lemmatizer', <spacy.lang.en.lemmatizer.EnglishLemmatizer at 0x1c6b30f3880>),\n",
       " ('ner', <spacy.pipeline.ner.EntityRecognizer at 0x1c6afbfed60>)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# íŒŒì´í”„ë¼ì¸ êµ¬ì„±ìš”ì†Œ í™•ì¸í•˜ê¸°\n",
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŒŒì„œ ë° ê°œì²´ëª… ì¸ì‹ ë¹„í™œì„±í™”\n",
    "# í† í°í™”ë§Œ ì›í•˜ë©´ í…ìŠ¤íŠ¸ì—ì„œ nlp.make_doc í˜¸ì¶œ\n",
    "nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### í…ìŠ¤íŠ¸ ì²˜ë¦¬"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My|best|friend|Ryan|Peters|likes|fancy|adventure|games|.|"
     ]
    }
   ],
   "source": [
    "for token in doc : \n",
    "    print(token, end='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# í† í°ê³¼ ê·¸ ì†ì„±ì„ í¬í•¨í•˜ëŠ” í…Œì´ë¸” ìƒì„±í•˜ëŠ” í•¨ìˆ˜\n",
    "# lemma : ì›í˜•ê³¼ ê´€ë ¨\n",
    "def display_nlp(doc, include_punct=False) : \n",
    "    '''ìŠ¤í˜ì´ì‹œ í† í°ë“¤ì˜ ì‹œê°í™”ë¥¼ ìœ„í•œ ë°ì´í„°í”„ë ˆì„ ìƒì„±í•˜ê¸°'''\n",
    "    rows = []\n",
    "    for i, t in enumerate(doc) : \n",
    "        if not t.is_punct or include_punct : \n",
    "            row = {'token':i, 'text':t.text, 'lemma_':t.lemma_,\n",
    "                   'is_stop':t.is_stop, 'is_alpha':t.is_alpha,\n",
    "                   'pos_':t.pos_, 'dep_':t.dep_,\n",
    "                   'ent_type':t.ent_type_, 'ent_iob':t.ent_iob_}\n",
    "            rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows).set_index('token')\n",
    "    df.index.name = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>lemma_</th>\n",
       "      <th>is_stop</th>\n",
       "      <th>is_alpha</th>\n",
       "      <th>pos_</th>\n",
       "      <th>dep_</th>\n",
       "      <th>ent_type</th>\n",
       "      <th>ent_iob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>My</td>\n",
       "      <td>my</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>PRON</td>\n",
       "      <td>poss</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>best</td>\n",
       "      <td>good</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>friend</td>\n",
       "      <td>friend</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>nsubj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Ryan</td>\n",
       "      <td>Ryan</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>compound</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Peters</td>\n",
       "      <td>Peters</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>PROPN</td>\n",
       "      <td>appos</td>\n",
       "      <td>PERSON</td>\n",
       "      <td>I</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>likes</td>\n",
       "      <td>like</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>VERB</td>\n",
       "      <td>ROOT</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>fancy</td>\n",
       "      <td>fancy</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>ADJ</td>\n",
       "      <td>amod</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>adventure</td>\n",
       "      <td>adventure</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>compound</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>games</td>\n",
       "      <td>game</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>NOUN</td>\n",
       "      <td>dobj</td>\n",
       "      <td></td>\n",
       "      <td>O</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        text     lemma_  is_stop  is_alpha   pos_      dep_ ent_type ent_iob\n",
       "0         My         my     True      True   PRON      poss                O\n",
       "1       best       good    False      True    ADJ      amod                O\n",
       "2     friend     friend    False      True   NOUN     nsubj                O\n",
       "3       Ryan       Ryan    False      True  PROPN  compound   PERSON       B\n",
       "4     Peters     Peters    False      True  PROPN     appos   PERSON       I\n",
       "5      likes       like    False      True   VERB      ROOT                O\n",
       "6      fancy      fancy    False      True    ADJ      amod                O\n",
       "7  adventure  adventure    False      True   NOUN  compound                O\n",
       "8      games       game    False      True   NOUN      dobj                O"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ì‚¬ìš©ì ì „ëµ í† í°í™”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Pete|:|choose|low|-|carb|#|food|#|eat|-|smart|.|_|url|_|;-)|ğŸ˜‹|ğŸ‘|"
     ]
    }
   ],
   "source": [
    "# ì˜ˆì‹œ\n",
    "text = \"@Pete: choose low-carb #food #eat-smart. _url_ ;-) ğŸ˜‹ğŸ‘\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for token in doc : \n",
    "    print(token, end='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì¤‘ìœ„, ì ‘ë‘ì‚¬, ì ‘ë¯¸ì‚¬ ë¶„í• ì— ëŒ€í•œ ê°œë³„ ê·œì¹™ ì ìš© í•¨ìˆ˜\n",
    "def custom_tokenizer(nlp) : \n",
    "    # re.searchì™€ ì¼ì¹˜í•˜ëŠ” íŒ¨í„´ì„ ì œì™¸í•˜ê³  ê¸°ë³¸ íŒ¨í„´ì„ ì‚¬ìš©í•œë‹¤.\n",
    "    prefixes = [pattern for pattern in nlp.Defaults.prefixes \n",
    "                if pattern not in ['-', '_', '#']]\n",
    "    suffixes = [pattern for pattern in nlp.Defaults.suffixes\n",
    "                if pattern not in ['_']]\n",
    "    infixes  = [pattern for pattern in nlp.Defaults.infixes\n",
    "                if not re.search(pattern, 'xx-xx')]\n",
    "\n",
    "    return Tokenizer(vocab          = nlp.vocab, \n",
    "                     rules          = nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search  = compile_prefix_regex(prefixes).search,\n",
    "                     suffix_search  = compile_suffix_regex(suffixes).search,\n",
    "                     infix_finditer = compile_infix_regex(infixes).finditer,\n",
    "                     token_match    = nlp.Defaults.token_match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.tokenizer = custom_tokenizer(nlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@Pete|:|choose|low-carb|#food|#eat-smart|.|_url_|;-)|ğŸ˜‹|ğŸ‘|"
     ]
    }
   ],
   "source": [
    "doc = nlp(text)\n",
    "for token in doc : \n",
    "    print(token, end='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ë¶ˆìš©ì–´ ì œê±°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Dear, Ryan, need, sit, talk, Regards, Pete]\n"
     ]
    }
   ],
   "source": [
    "text = \"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
    "doc = nlp(text)\n",
    "\n",
    "non_stop = [t for t in doc if not t.is_stop and not t.is_punct]\n",
    "print(non_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "nlp.vocab['down'].is_stop = False\n",
    "nlp.vocab['Dear'].is_stop = True\n",
    "nlp.vocab['Regrads'].is_stop = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Ryan, need, sit, down, talk, Regards, Pete]\n"
     ]
    }
   ],
   "source": [
    "text = \"Dear Ryan, we need to sit down and talk. Regards, Pete\"\n",
    "doc = nlp(text)\n",
    "\n",
    "non_stop = [t for t in doc if not t.is_stop and not t.is_punct]\n",
    "print(non_stop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### í•´ë‹¹ ì–¸ì–´ í´ë˜ìŠ¤ì˜ í•˜ìœ„ í´ë˜ìŠ¤ë¥¼ ìƒì„±í•˜ê³  ê±°ê¸°ì— ìì‹ ì˜ ë¶ˆìš©ì–´ ëª©ë¡ ë§Œë“¤ê¸°(ìŠ¤í˜ì´ì‹œ 3.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ryan|need|sit|down|talk|Pete|"
     ]
    }
   ],
   "source": [
    "excluded_stop_words = {'down'}\n",
    "included_stop_words = {'dear','regards'}\n",
    "\n",
    "class CustomEnglishDefaults(English.Defaults) : \n",
    "    stop_words = English.Defaults.stop_words.copy()\n",
    "    stop_words -= excluded_stop_words\n",
    "    stop_words |= included_stop_words # |= : in place setì—°ì‚°ì(í•©ì§‘í•©ì„ êµ¬í•œ í›„ ì›ë˜ ì§‘í•©ì— ê²°ê³¼ í• ë‹¹)\n",
    "\n",
    "class CustomEnglish(English) : \n",
    "    Defaults = CustomEnglishDefaults\n",
    "\n",
    "nlp = CustomEnglish()\n",
    "\n",
    "text = 'Dear Ryan, we need to sit down and talk. Regards, Pete'\n",
    "doc = nlp.make_doc(text)  # only tokenize\n",
    "\n",
    "tokens_wo_top = [token for token in doc]\n",
    "for token in doc : \n",
    "    if not token.is_stop and not token.is_punct : \n",
    "        print(token, end='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset nlp to original\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### í’ˆì‚¬ ê¸°ë°˜ ì›í˜• ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "my|good|friend|Ryan|Peters|like|fancy|adventure|game|.\n"
     ]
    }
   ],
   "source": [
    "# ì›í˜• ë³µì› : lemma_ì†ì„±ìœ¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥\n",
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "print(*[t.lemma_ for t in doc], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[friend, Ryan, Peters, adventure, games]\n"
     ]
    }
   ],
   "source": [
    "# ë¬¸ì„œì— ëª…ì‚¬ì™€ ê³ ìœ ëª…ì‚¬ë§Œ ìˆëŠ” ëª©ë¡ ìƒì„±í•˜ê¸°\n",
    "text = \"My best friend Ryan Peters likes fancy adventure games.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "nouns = [t for t in doc if t.pos_ in ['NOUN','PROPN']]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best|friend|fancy|adventure|games\n"
     ]
    }
   ],
   "source": [
    "# ìƒ˜í”Œ ë¬¸ì¥ì—ì„œ í˜•ìš©ì‚¬ì™€ ëª…ì‚¬ì˜ í† í° ì¶”ì¶œí•˜ê¸°\n",
    "tokens = textacy.extract.words(doc,\n",
    "                               filter_stops = True,           # ê¸°ë³¸ì€ True(ë¶ˆìš©ì–´ í—ˆìš© ì•ˆí•¨.)\n",
    "                               filter_punct = True,           # ê¸°ë³¸ì€ True(êµ¬ë‘ì ì„ í—ˆìš© ì•ˆí•¨.)\n",
    "                               filter_nums = True,            # ê¸°ë³¸ì€ False\n",
    "                               include_pos = ['ADJ', 'NOUN'], # ê¸°ë³¸ì€ None(ëª¨ë“  í’ˆì‚¬ í—ˆìš©)\n",
    "                               exclude_pos = None,            # ê¸°ë³¸ì€ None(ëª¨ë“  í’ˆì‚¬ë¥¼ ë°°ì œí•˜ì§€ ì•ŠìŒ.)\n",
    "                               min_freq=1)\n",
    "\n",
    "print(*[t for t in tokens], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good|friend|fancy|adventure|game\n"
     ]
    }
   ],
   "source": [
    "# ë‹¨ì–´ì˜ ì›í˜• ëª©ë¡ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜\n",
    "def extract_lemmas(doc, **kwargs) : \n",
    "    return [t.lemma_ for t in textacy.extract.words(doc, **kwargs)]\n",
    "\n",
    "lemmas = extract_lemmas(doc, include_pos=['ADJ','NOUN'])\n",
    "print(*lemmas, sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ëª…ì‚¬êµ¬ ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good friend|fancy adventure|fancy adventure game\n"
     ]
    }
   ],
   "source": [
    "# í˜•ìš©ì‚¬ê°€ ì•ì— ìˆëŠ” ëª…ì‚¬ ì‹œí€€ìŠ¤ ì¶”ì¶œí•˜ê¸°\n",
    "text = 'My best friend Ryan Peters likes fancy adventure games.'\n",
    "doc = nlp(text)\n",
    "\n",
    "patterns = [\"POS:ADJ POS:NOUN:+\"]\n",
    "\n",
    "if textacy.__version__ < '0.11':\n",
    "    # as in book\n",
    "    spans = textacy.extract.matches(doc, patterns=patterns)\n",
    "else:\n",
    "    # new textacy version\n",
    "    spans = textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "    \n",
    "# spans = textacy.extract.matches(doc, patterns=patterns)\n",
    "print(*[s.lemma_ for s in spans], sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My best friend|Ryan Peters|fancy adventure games\n"
     ]
    }
   ],
   "source": [
    "# ëª…ì‚¬êµ¬ ì¶”ì¶œ(ëŒ€ëª…ì‚¬ì™€ í•œì •ì‚¬ë„ í¬í•¨ë  ìˆ˜ ìˆìŒ)\n",
    "print(*doc.noun_chunks, sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "good_friend|fancy_adventure|fancy_adventure_game|adventure_game\n"
     ]
    }
   ],
   "source": [
    "# í’ˆì‚¬ íŒ¨í„´ì„ ê¸°ë°˜ìœ¼ë¡œ ëª…ì‚¬êµ¬ ì¶”ì¶œí•˜ê¸°(êµ¬ë¶„ ë¬¸ì ì‚¬ìš©)\n",
    "# í˜•ìš©ì‚¬ë‚˜ ëª…ì‚¬ ë’¤ì— ëª…ì‚¬ê°€ ì˜¤ëŠ” êµ¬ì ˆ ì¶”ì¶œí•˜ê¸°\n",
    "def extract_noun_phrases(doc, preceding_pos=['NOUN'], sep='_'):\n",
    "    patterns = []\n",
    "    for pos in preceding_pos:\n",
    "        patterns.append(f\"POS:{pos} POS:NOUN:+\")\n",
    "\n",
    "    if textacy.__version__ < '0.11':\n",
    "        # as in book\n",
    "        spans = textacy.extract.matches(doc, patterns=patterns)\n",
    "    else:\n",
    "        # new textacy version\n",
    "        spans = textacy.extract.matches.token_matches(doc, patterns=patterns)\n",
    "\n",
    "    return [sep.join([t.lemma_ for t in s]) for s in spans]\n",
    "\n",
    "print(*extract_noun_phrases(doc, ['ADJ', 'NOUN']), sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ê°œì²´ëª… ì¶”ì¶œ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(James O'Neill, PERSON) (World Cargo Inc, ORG) (San Francisco, GPE) "
     ]
    }
   ],
   "source": [
    "# label_ : ê°œì²´ ìœ í˜•ì— ëŒ€í•œ ì†ì„±\n",
    "text = \"James O'Neill, chairman of World Cargo Inc, lives in San Francisco.\"\n",
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents : \n",
    "    print(f'({ent.text}, {ent.label_})', end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    James O'Neill\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ", chairman of \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    World Cargo Inc\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", lives in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    San Francisco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# displacy : ê°œì²´ëª… ì¸ì‹ì„ ìœ„í•œ ì‹œê°í™” ì œê³µ\n",
    "displacy.render(doc, style='ent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# íŠ¹ì • ìœ í˜•ì˜ ëª…ëª…ëœ ê°œì²´ë¥¼ ì¶”ì¶œí•˜ê¸°\n",
    "def extract_entities(doc, include_types=None, sep='_') : \n",
    "    ents = textacy.extract.entities(doc,\n",
    "                                    include_types=include_types,\n",
    "                                    exclude_types=None,\n",
    "                                    drop_determiners=True,\n",
    "                                    min_freq=1)\n",
    "    \n",
    "    return [sep.join([t.lemma_ for t in e])+'/'+e.label_ for e in ents]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
